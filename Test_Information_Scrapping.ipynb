{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqWBfWjUZ9MWJJCObHUH5c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudenurcure/WebScrapping/blob/main/Test_Information_Scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd-v6J5b_a2b"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import csv\n",
        "\n",
        "BASE_URL = \"https://www.mayocliniclabs.com/test-catalog/alphabetical/\"\n",
        "TEST_BASE_URL = \"https://www.mayocliniclabs.com\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "def fetch_html(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_test_links(page_html):\n",
        "    soup = BeautifulSoup(page_html, \"html.parser\")\n",
        "    section = soup.find(\"div\", class_=\"rochester-section\")\n",
        "    if section:\n",
        "        links = section.find_all(\"a\", href=True)\n",
        "        return [TEST_BASE_URL + link[\"href\"] for link in links]\n",
        "    return []\n",
        "\n",
        "def parse_test_details(test_html):\n",
        "    soup = BeautifulSoup(test_html, \"html.parser\")\n",
        "    test_details = {}\n",
        "\n",
        "    # Extract test name\n",
        "    test_info = soup.find(\"div\", class_=\"test-info\")\n",
        "    if test_info and test_info.h2:\n",
        "        test_details[\"Test Name\"] = test_info.h2.get_text(strip=True)\n",
        "\n",
        "    # Extract tab content\n",
        "    tabs = [\"Overview\", \"Specimen\", \"Clinical-and-Interpretive\", \"Performance\"]\n",
        "    for tab in tabs:\n",
        "        tab_content = soup.find(\"div\", id=f\"tabcontent-{tab}\")\n",
        "        if tab_content:\n",
        "            test_details[tab] = tab_content.get_text(strip=True)\n",
        "\n",
        "    return test_details\n",
        "\n",
        "def save_to_csv(data, filename=\"tests_data.csv\"):\n",
        "    keys = data[0].keys() if data else []\n",
        "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "def main():\n",
        "    test_data = []\n",
        "    for letter in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
        "        url = f\"{BASE_URL}{letter}\"\n",
        "        print(f\"Fetching page: {url}\")\n",
        "        page_html = fetch_html(url)\n",
        "        if page_html:\n",
        "            test_links = parse_test_links(page_html)\n",
        "            print(f\"Found {len(test_links)} tests on {letter} page.\")\n",
        "\n",
        "            for test_url in test_links:\n",
        "                print(f\"Fetching test: {test_url}\")\n",
        "                test_html = fetch_html(test_url)\n",
        "                if test_html:\n",
        "                    test_details = parse_test_details(test_html)\n",
        "                    test_data.append(test_details)\n",
        "                    time.sleep(5)  # Respect crawl delay\n",
        "\n",
        "    # Save data to CSV\n",
        "    save_to_csv(test_data)\n",
        "    print(\"Data scraping completed and saved to tests_data.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}